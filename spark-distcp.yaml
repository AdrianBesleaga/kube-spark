apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: spark-distcp
  namespace: default
spec:
  type: Scala
  mode: cluster
  image: "uprush/apache-spark:2.4.5"
  imagePullPolicy: Always
  mainClass: com.coxautodata.SparkDistCP
  mainApplicationFile: "s3a://deephub/user/pureuser/spark-distcp/spark-distcp_2.12-0.2.2.jar"
  arguments: ["--dryrun", "s3a://deephub/warehouse/doglover", "s3a://bigdatabucket/warehouse/"]
  sparkVersion: "2.4.5"
  restartPolicy:
    type: Never
  volumes:
    - name: "staging-vol"
      persistentVolumeClaim:
        claimName: data-staging-share
  driver:
    cores: 2
    coreLimit: "1200m"
    memory: "512m"
    volumeMounts:
      - name: "staging-vol"
        mountPath: "/home/spark/tmp"
    labels:
      version: "2.4.5"
    serviceAccount: spark
  executor:
    cores: 2
    instances: 20
    memory: "2048m"
    volumeMounts:
      - name: "staging-vol"
        mountPath: "/home/spark/tmp"
    labels:
      version: "2.4.5"
  # Workaround for issue #216
  sparkConf:
    "spark.hadoop.fs.s3a.bucket.deephub.endpoint": "192.168.170.12"
    "spark.hadoop.fs.s3a.bucket.deephub.access.key": "S3_ACCESS_KEY"
    "spark.hadoop.fs.s3a.bucket.deephub.secret.key": "S3_SECRET_KEY"
    "spark.hadoop.fs.s3a.bucket.bigdatabucket.endpoint": "s3-singapore.purestorage.int"
    "spark.hadoop.fs.s3a.bucket.bigdatabucket.access.key": "S3_ACCESS_KEY"
    "spark.hadoop.fs.s3a.bucket.bigdatabucket.secret.key": "S3_SECRET_KEY"
    "spark.hadoop.fs.s3a.connection.ssl.enabled": "false"
    "spark.hadoop.fs.s3a.fast.upload": "true"
    "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version": "2"
    "spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored": "true"
    "spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a": "org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory"
    "spark.hadoop.fs.s3a.committer.name": "directory"
    "spark.sql.sources.commitProtocolClass": "org.apache.spark.internal.io.cloud.PathOutputCommitProtocol"
    "spark.sql.parquet.output.committer.class": "org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter"
    "spark.hadoop.fs.s3a.committer.tmp.path": "file:///home/spark/tmp/staging"
    "spark.hadoop.fs.s3a.buffer.dir": "/home/spark/tmp/buffer"
    "spark.eventLog.dir": "s3a://deephub/spark/spark-events/"
    "spark.eventLog.enabled": "true"
    "spark.sql.warehouse.dir": "s3a://deephub/spark/warehouse"
    "spark.hadoop.parquet.enable.summary-metadata": "false"
    "spark.sql.parquet.mergeSchema": "false"
    "spark.sql.parquet.filterPushdown": "true"
    "spark.sql.hive.metastorePartitionPruning": "true"
    "spark.sql.orc.filterPushdown": "true"
    "spark.sql.orc.splits.include.file.footer": "true"
    "spark.sql.orc.cache.stripe.details.size": "10000"
