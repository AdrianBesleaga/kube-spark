apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: doglover
  namespace: default
spec:
  type: Scala
  mode: cluster
  image: "uprush/apache-spark:2.4.5"
  imagePullPolicy: Always
  mainClass: com.uprush.example.DogLover
  mainApplicationFile: "s3a://deephub/user/yijiang/doglover/doglover_2.12-0.1.0-SNAPSHOT.jar"
  sparkVersion: "2.4.5"
  restartPolicy:
    type: Never
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    labels:
      version: "2.4.5"
    serviceAccount: spark
  executor:
    cores: 1
    instances: 2
    memory: "512m"
    labels:
      version: "2.4.5"
  # Workaround for issue #216
  sparkConf:
    "spark.hadoop.fs.s3a.endpoint": "192.168.170.12"
    "spark.hadoop.fs.s3a.access.key": "S3_ACCESS_KEY"
    "spark.hadoop.fs.s3a.secret.key": "S3_SECRET_KEY"
    "spark.hadoop.fs.s3a.connection.ssl.enabled": "false"
    "spark.hadoop.fs.s3a.fast.upload": "true"
    "spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a": "org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory"
    "spark.hadoop.fs.s3a.committer.name": "directory"
    "spark.hadoop.fs.s3a.committer.tmp.path": "tmp/staging"
    "spark.eventLog.dir": "s3a://deephub/spark/spark-events/"
    "spark.eventLog.enabled": "true"
    "spark.sql.warehouse.dir": "s3a://deephub/spark/warehouse"
    "spark.hadoop.parquet.enable.summary-metadata": "false"
    "spark.sql.parquet.mergeSchema": "false"
    "spark.sql.parquet.filterPushdown": "true"
    "spark.sql.hive.metastorePartitionPruning": "true"
    "spark.sql.orc.filterPushdown": "true"
    "spark.sql.orc.splits.include.file.footer": "true"
    "spark.sql.orc.cache.stripe.details.size": "10000"
